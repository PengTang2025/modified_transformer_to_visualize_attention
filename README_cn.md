# Transformer Attention Visualization

æœ¬é¡¹ç›®æ—¨åœ¨**ä¾¿æ·æå– Transformer æ¨¡å‹ä¸­çš„æ³¨æ„åŠ›æƒé‡ï¼ˆAttention Weightsï¼‰ç”¨äºå¯è§†åŒ–ä¸åˆ†æ**ã€‚ä¸ºæ­¤ï¼Œæˆ‘è‡ªå®šä¹‰äº†ä¸¤ä¸ªåŸºäº PyTorch çš„æ´¾ç”Ÿç±»ï¼Œåˆ†åˆ«ç”¨äº Encoder å’Œ Decoder å±‚ï¼Œæ‰©å±•å…¶æ³¨æ„åŠ›æå–èƒ½åŠ›ã€‚

## âœ¨ åŠŸèƒ½ç‰¹ç‚¹

- âœ… æ”¯æŒæå– TransformerEncoderLayer æ¯å±‚çš„ self-attention æƒé‡
- âœ… æ”¯æŒæå– TransformerDecoderLayer ä¸­çš„ï¼š
  - masked self-attentionï¼ˆè‡ªå›å½’ï¼‰
  - cross-attentionï¼ˆè·¨åºåˆ—å¯¹é½ï¼‰
- âœ… ä¸ç ´å PyTorch åŸæœ‰ç»“æ„ä¸ forward é€»è¾‘ï¼Œå…¼å®¹æ€§å¥½
- âœ… æ”¯æŒå¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰å¯è§†åŒ–

## ğŸ§© å®ç°æ–¹å¼

ç”±äº PyTorch å®˜æ–¹çš„ `nn.TransformerEncoderLayer` å’Œ `nn.TransformerDecoderLayer` é»˜è®¤ **ä¸è¿”å›æ³¨æ„åŠ›æƒé‡**ï¼Œæœ¬é¡¹ç›®é€šè¿‡**ç»§æ‰¿æ–¹å¼**é‡å†™äº† `_sa_block()`ï¼ˆself-attentionï¼‰å’Œ `_mha_block()`ï¼ˆmulti-head/cross attentionï¼‰ç­‰å†…éƒ¨æ–¹æ³•ï¼Œä»è€Œï¼š

- ä¿ç•™åŸå§‹ forward æ¥å£ï¼›
- åœ¨æ¯æ¬¡å‰å‘ä¼ æ’­æ—¶è‡ªåŠ¨ä¿å­˜ `attn_weights` åˆ°ç±»å±æ€§ä¸­ã€‚

è¿™æ ·å¯ç›´æ¥é€šè¿‡ `.attn_weights` è®¿é—®æ¯å±‚æ³¨æ„åŠ›çŸ©é˜µã€‚


## ğŸ” ç¤ºä¾‹

å®Œæ•´ä»£ç ä¸è®­ç»ƒé€»è¾‘å¯æŸ¥[transformer_copy project](https://github.com/PengTang2025/transformer_copy)

```python

from coderlayer_with_attn import TransformerEncoderLayerWithAttn, TransformerDecoderLayerWithAttn

class TransformerXXXModel(nn.Module):
  def __init__(...):
    ...
    self.last_attn = None
    ...
  ...
  encoder_layer = TransformerEncoderLayerWithAttn(...)
  self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)
  ...

  def forward(...):
    self.last_attn = self.encoder.layers[-1].attn_weights
    ...

#visualize
def plot_attention_weights(model, sample_src, sample_input):
    _ = model(sample_src, sample_input)  
    attn_weights = model.last_attn  
    ...

# train
model = TransformerXXXModel(...)
...
model.eval() # æ­¤å¤„ä¸å¯åŒæ—¶ä½¿ç”¨model.eval()å’Œwith torch.no_grad()ï¼Œè¯¦è§ä¸‹æ–‡æ³¨æ„äº‹é¡¹
plot_attention_weights(model, ...)
```
## âš ï¸ æ³¨æ„äº‹é¡¹ï¼šé’ˆå¯¹ Encoder æ³¨æ„åŠ›æå–
ï¼ˆæˆªè‡³ PyTorch 2.7ï¼‰å…³äº eval() ä¸ no_grad() çš„ä½¿ç”¨ï¼Œè¯·ç‰¹åˆ«æ³¨æ„ï¼š  
â— è‹¥åŒæ—¶å¯ç”¨ with torch.no_grad() å’Œ model.eval()ï¼ŒEncoder çš„æ³¨æ„åŠ›æå–å°†å¤±æ•ˆï¼  
è¿™æ˜¯ç”±äº PyTorch åœ¨ TransformerEncoderLayer ä¸­å¯ç”¨äº†ç¨€ç–è®¡ç®—è·¯å¾„ï¼Œå½“ç¬¦åˆæ¡ä»¶æ—¶ä¼šç›´æ¥return torch._transformer_encoder_layer_fwd() ç»“æŸ forward(), è·³è¿‡åç»­çš„ _sa_block() è°ƒç”¨ï¼Œå¯¼è‡´ self.attn_weights = Noneã€‚ï¼ˆè¯¦è§ why_not_sparsity_fast_path å˜é‡çš„ä¸€ç³»åˆ—é€»è¾‘ï¼Œå®ƒæ˜¯ PyTorch Transformer æ¨¡å—å†…éƒ¨ç”¨äºæ§åˆ¶æ˜¯å¦ä½¿ç”¨ç¨€ç–è®¡ç®—è·¯å¾„çš„æ ‡å¿—ï¼Œåœ¨ forward() å‡½æ•°çš„æœ€å‰ç«¯ï¼‰  
TransformerDecoderLayer æ— ç¨€ç–è®¡ç®—è·¯å¾„ï¼Œä¸å—æ­¤é™åˆ¶ã€‚
å½“with torch.no_grad() å’Œ model.eval()åªèƒ½é€‰å…¶ä¸€æ—¶ï¼Œé‰´äºæˆ‘ä»¬æ›´å¸Œæœ›åœ¨æµ‹è¯•ä¸ç»˜å›¾ä¸­ä¸ä½¿ç”¨dropout/batchnormï¼Œå»ºè®®åœ¨ä½¿ç”¨æ—¶å…ˆå®Œæˆæœ€ç»ˆæ¨¡å‹çš„ä¿å­˜ï¼Œç¡®ä¿å¯è§†åŒ–ä½¿ç”¨çš„æ•°æ®ä¸å†æ›´æ–°æ¨¡å‹ï¼Œå†åœ¨ eval() æ¨¡å¼ä¸‹è¿›è¡Œæ³¨æ„åŠ›å¯è§†åŒ–ç»˜å›¾ã€‚


## ğŸ“œ License
MIT License. Â© 2025 PengTang
