# Transformer Attention Visualization

æœ¬é¡¹ç›®æ—¨åœ¨**ä¾¿æ·æå– Transformer æ¨¡å‹ä¸­çš„æ³¨æ„åŠ›æƒé‡ï¼ˆAttention Weightsï¼‰ç”¨äºå¯è§†åŒ–ä¸åˆ†æ**ã€‚ä¸ºæ­¤ï¼Œæˆ‘è‡ªå®šä¹‰äº†ä¸¤ä¸ªåŸºäº PyTorch çš„æ´¾ç”Ÿç±»ï¼Œåˆ†åˆ«ç”¨äº Encoder å’Œ Decoder å±‚ï¼Œæ‰©å±•å…¶æ³¨æ„åŠ›æå–èƒ½åŠ›ã€‚

## âœ¨ åŠŸèƒ½ç‰¹ç‚¹

- âœ… æ”¯æŒæå– TransformerEncoderLayer æ¯å±‚çš„ self-attention æƒé‡
- âœ… æ”¯æŒæå– TransformerDecoderLayer ä¸­çš„ï¼š
  - masked self-attentionï¼ˆè‡ªå›å½’ï¼‰
  - cross-attentionï¼ˆè·¨åºåˆ—å¯¹é½ï¼‰
- âœ… ä¸ç ´å PyTorch åŸæœ‰ç»“æ„ä¸ forward é€»è¾‘ï¼Œå…¼å®¹æ€§å¥½
- âœ… æ”¯æŒå¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰å¯è§†åŒ–

## ğŸ§© å®ç°æ–¹å¼

ç”±äº PyTorch å®˜æ–¹çš„ `nn.TransformerEncoderLayer` å’Œ `nn.TransformerDecoderLayer` é»˜è®¤ **ä¸è¿”å›æ³¨æ„åŠ›æƒé‡**ï¼Œæœ¬é¡¹ç›®é€šè¿‡**ç»§æ‰¿æ–¹å¼**é‡å†™äº† `_sa_block()`ï¼ˆself-attentionï¼‰å’Œ `_mha_block()`ï¼ˆmulti-head/cross attentionï¼‰ç­‰å†…éƒ¨æ–¹æ³•ï¼Œä»è€Œï¼š

- ä¿ç•™åŸå§‹ forward æ¥å£ï¼›
- åœ¨æ¯æ¬¡å‰å‘ä¼ æ’­æ—¶è‡ªåŠ¨ä¿å­˜ `attn_weights` åˆ°ç±»å±æ€§ä¸­ã€‚

è¿™æ ·å¯ç›´æ¥é€šè¿‡ `.attn_weights` è®¿é—®æ¯å±‚æ³¨æ„åŠ›çŸ©é˜µã€‚


## ğŸ” ç¤ºä¾‹

å®Œæ•´ä»£ç ä¸è®­ç»ƒé€»è¾‘å¯æŸ¥transformer_copy project

URL:https://github.com/PengTang2025/transformer_copy

```python

from coderlayer_with_attn import TransformerEncoderLayerWithAttn, TransformerDecoderLayerWithAttn

class TransformerXXXModel(nn.Module):
  def __init__(...):
    ...
    self.last_attn = None
    ...
  ...
  encoder_layer = TransformerEncoderLayerWithAttn(...)
  self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)
  ...

  def forward(...):
    self.last_attn = self.encoder.layers[-1].attn_weights
    ...

#visualize
def plot_attention_weights(model, sample_src, sample_input):
    _ = model(sample_src, sample_input)  
    attn_weights = model.last_attn  
    ...

# train
model = TransformerXXXModel(...)
...
plot_attention_weights(model, ...)

## ğŸ“œ License
MIT License. Â© 2025 PengTang
